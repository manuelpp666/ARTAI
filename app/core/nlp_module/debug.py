import os
from preprocess import construir_vocab
import json

# ----------------------
# Rutas
# ----------------------
ruta_dataset = os.path.join(os.path.dirname(__file__), "../../../datasets/espa침ol/arte_traducido/dataset_completo1.txt")
ruta_vocab = "bpe_tokenizer.json"

# ----------------------
# Construir / cargar tokenizer
# ----------------------
print("游댳 Construyendo tokenizer...")
tokenizer, stoi, itos = construir_vocab(ruta_dataset, ruta_vocab=ruta_vocab, vocab_size=10000)

# ----------------------
# Chequeo tokens especiales
# ----------------------
tokens_especiales = ["SECCION", "[FIN_SECCION]"]
for t in tokens_especiales:
    token_ids = tokenizer.encode(t).ids
    print(f"Token '{t}' codificado como IDs: {token_ids}")
    if len(token_ids) != 1:
        print(f"丘멆잺 ATENCI칍N: '{t}' no es un token 칰nico. Esto puede causar errores de generaci칩n.")

# ----------------------
# Chequeo del dataset
# ----------------------
print("\n游댳 Analizando dataset...")
with open(ruta_dataset, "r", encoding="utf-8") as f:
    lineas = f.readlines()

longitudes = [len(line.split()) for line in lineas]
print(f"Total de l칤neas: {len(lineas)}")
print(f"Longitud m칤nima de l칤nea: {min(longitudes)} palabras")
print(f"Longitud m치xima de l칤nea: {max(longitudes)} palabras")
print(f"Longitud promedio de l칤nea: {sum(longitudes)/len(longitudes):.2f} palabras")

# Mostrar ejemplos de l칤neas largas (>500 palabras)
print("\n游댳 Ejemplos de l칤neas largas (>500 palabras):")
for i, l in enumerate(lineas):
    if len(l.split()) > 500:
        print(f"- L칤nea {i}: {len(l.split())} palabras")
        print("  ", l[:200], "...\n")  # Muestra primeros 200 caracteres

# ----------------------
# Chequeo de delimitador [FIN_SECCION]
# ----------------------
count_fin_seccion = sum(1 for l in lineas if "[FIN_SECCION]" in l)
print(f"\nSecciones con [FIN_SECCION]: {count_fin_seccion} de {len(lineas)} l칤neas")
if count_fin_seccion != len(lineas):
    print("丘멆잺 Algunas l칤neas no contienen '[FIN_SECCION]'. Esto puede romper la generaci칩n autoregresiva.")
